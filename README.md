# Statistical-NLP-Coursework
CNN Branch
NER Reference Repo: https://github.com/niccolot/NER_biLSTM-CNN?tab=readme-ov-file
Embedding: https://github.com/ncbi-nlp/BioWordVec?tab=readme-ov-file



ğŸ§¬ CNN based BC5CDR NERâ€“RE Pipeline

Two-stage biomedical information extraction pipeline:

PubTator â†’ NER (Chemical/Disease) â†’ RE (CID) â†’ Document-level CID prediction

Supports:
	â€¢	Independent NER evaluation
	â€¢	Independent RE evaluation (gold entities)
	â€¢	Full end-to-end pipeline evaluation (predicted entities)

â¸»

ğŸ“‚ Project Structure

Statistical-NLP-Coursework/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ origin/                    # Raw BC5CDR PubTator files
â”‚   â””â”€â”€ processed/                 # Sentence-level CoNLL + sentence index
â”‚
â”œâ”€â”€ embeddings/
â”‚   â””â”€â”€ biowordvec_bc5_subset.txt  # 200d biomedical embeddings
â”‚
â”œâ”€â”€ NER_biLSTM-CNN-master/         # NER model (BiLSTM + CNN-char)
â”‚
â”œâ”€â”€ PrepareREData.py               # Build RE gold training data
â”œâ”€â”€ build_sentence_index.py        # Align sentence-level NER back to PMID
â”œâ”€â”€ RE_Test_Pred.py                # Full NERâ†’RE pipeline inference
â”‚
â””â”€â”€ re_data_gold/                  # RE gold dataset (auto-generated)


â¸»

1ï¸âƒ£ NER Module

Model
	â€¢	BiLSTM + CNN-char
	â€¢	Word embeddings: BioWordVec (200d)

Training Data

data/processed/bc5cdr_ner_sentence_{train,dev,test}.txt

Sentence-level CoNLL format:

token   X   X   B-CHEM
token   X   X   O
(blank line = sentence boundary)

Train

Inside NER_biLSTM-CNN-master/:

python train.py

Model saved to:

models/bilstm_cnn_bc5_sentence.keras


â¸»

2ï¸âƒ£ Embeddings

Using domain-specific biomedical embeddings:

embeddings/biowordvec_bc5_subset.txt

	â€¢	200-dimensional
	â€¢	Subset extracted from BioWordVec
	â€¢	Initialized as non-trainable

â¸»

3ï¸âƒ£ RE Gold Data Preparation

Script:

python PrepareREData.py

Input:

data/origin/CDR_*Set.PubTator.txt

Output:

re_data_gold/
  re_train_gold.tsv
  re_dev_gold.tsv
  re_test_gold.tsv

Format:

LABEL \t sentence_with_<e1>_<e2>_markers

Example:

CID    <e1> aspirin </e1> reduces <e2> headache </e2>

Used to train RE model with gold entities.

â¸»

4ï¸âƒ£ Sentence Index (NERâ€“RE Bridge)

Script:

python build_sentence_index.py

Output:

data/processed/
  bc5cdr_sentence_index_{train,dev,test}.jsonl

Each record:

{
  "pmid": "...",
  "sent_id": 3,
  "tokens": [...],
  "token_offsets": [...]
}

Purpose:
	â€¢	Align sentence-level NER output back to document level
	â€¢	Required for end-to-end pipeline

â¸»

5ï¸âƒ£ End-to-End Pipeline

Script:

python RE_Test_Pred.py

Steps:
	1.	Load trained NER model
	2.	Predict BIO tags on test set
	3.	Convert predicted entities back to document-level
	4.	Build RE test instances using predicted entities

Output:

pipeline_outputs/
  test_pred_entities.jsonl
  re_test_pred.tsv

This is the predicted-entity RE test set.

â¸»

ğŸ” Replacing the RE Model

NER is frozen.
You can plug in any RE model.

Training

Use:

re_data_gold/re_train_gold.tsv
re_data_gold/re_dev_gold.tsv

If your model requires:
	â€¢	Different format
	â€¢	Entity positions
	â€¢	Token indices

Use:

re_data_gold/re_train_gold.jsonl

which contains:
	â€¢	token list
	â€¢	chem_span
	â€¢	dis_span
	â€¢	mesh ids
	â€¢	pmid
	â€¢	sentence id

â¸»

Testing (Pipeline Evaluation)

Use:

pipeline_outputs/re_test_pred.tsv

Predict labels on this file to evaluate full pipeline performance.

â¸»

ğŸ“Š Evaluation Settings

We support three evaluation modes:

Setting	Description
NER only	BIO F1
RE (gold entities)	Upper bound
RE (predicted entities)	End-to-end pipeline


â¸»

ğŸ§  Design Principles

âœ” Modular

NER and RE are fully decoupled.

âœ” Replaceable RE

Any new RE model only needs to adapt to:

re_train_gold.tsv
re_test_pred.tsv

âœ” Error Propagation Analysis

Compare:
	â€¢	RE + gold entities
	â€¢	RE + predicted entities

to analyze NER impact.

â¸»

ğŸš€ Quick Start (Full Pipeline)

# 1. Train NER
cd NER_biLSTM-CNN-master
python train.py

# 2. Prepare RE gold data
cd ..
python PrepareREData.py

# 3. Build sentence index
python build_sentence_index.py

# 4. Train RE baseline (your model)

# 5. Run pipeline inference
python RE_Test_Pred.py


â¸»

ğŸ“Œ Current Status

âœ” NER trained
âœ” Biomedical embeddings integrated
âœ” RE gold dataset prepared
âœ” Sentence alignment implemented
âœ” End-to-end inference script ready

Next step: experiment with different RE models.

